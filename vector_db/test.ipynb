{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import ollama\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "MODEL_NAME = \"mistral-nemo\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "\n",
    "# 1Ô∏è‚É£ **Initialize Vector & Keyword (BM25) Retrievers**\n",
    "def create_hybrid_retriever():\n",
    "    # Load text documents\n",
    "    QA_doc = [\n",
    "        \"question: What is Artificial Intelligence (AI)?, answer: AI is the simulation of human intelligence in machines that can perform tasks such as learning, reasoning, and problem-solving.\",\n",
    "        \"question: What is the Turing Test?, answer: The Turing Test is a measure of a machine's ability to exhibit human-like intelligence, proposed by Alan Turing in 1950.\",\n",
    "        \"question: What is deep learning?, answer: Deep learning is a subset of machine learning that uses artificial neural networks to model and understand complex patterns in data.\",\n",
    "        \"question: What are the main types of machine learning?, answer: The three main types are supervised learning, unsupervised learning, and reinforcement learning.\",\n",
    "        \"question: What is blockchain technology?, answer: Blockchain is a decentralized digital ledger that records transactions across multiple computers securely and transparently.\",\n",
    "        \"question: What is the main ingredient in sushi?, answer: The main ingredient in sushi is vinegared rice, often paired with raw or cooked seafood, vegetables, and seaweed.\",\n",
    "        \"question: What is the world's hottest chili pepper?, answer: The Carolina Reaper is considered the world's hottest chili pepper, with an average of over 1.6 million Scoville Heat Units (SHU).\",\n",
    "        \"question: What is the difference between vegan and vegetarian diets?, answer: Vegetarians avoid meat, while vegans avoid all animal products, including dairy, eggs, and honey.\",\n",
    "        \"question: What is the capital of Japan?, answer: The capital of Japan is Tokyo.\",\n",
    "        \"question: Who developed the theory of relativity?, answer: Albert Einstein developed the theory of relativity.\",\n",
    "        \"question: What is the smallest unit of matter?, answer: The atom is the smallest unit of matter that retains the properties of an element.\",\n",
    "        \"question: What is photosynthesis?, answer: Photosynthesis is the process by which green plants use sunlight to synthesize food from carbon dioxide and water.\",\n",
    "        \"question: How many continents are there on Earth?, answer: There are seven continents on Earth: Africa, Antarctica, Asia, Europe, North America, Oceania, and South America.\",\n",
    "        \"question: What is the speed of light?, answer: The speed of light is approximately 299,792 kilometers per second (186,282 miles per second) in a vacuum.\",\n",
    "        \"question: What is the largest organ in the human body?, answer: The skin is the largest organ in the human body.\",\n",
    "        \"question: Who best in the world?, answer: Batman is storangest can fight to everyone.\"\n",
    "    ]\n",
    "\n",
    "    # **Vector-based Search**\n",
    "    ollama.pull(EMBEDDING_MODEL)\n",
    "    vector_db = Chroma.from_texts(QA_doc, embedding=OllamaEmbeddings(model=EMBEDDING_MODEL))\n",
    "    vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})  # Top 3 matches\n",
    "    logging.info(\"Vector database created.\")\n",
    "\n",
    "    # **Keyword-based Search (BM25)**\n",
    "    keyword_retriever = BM25Retriever.from_texts(QA_doc)\n",
    "    keyword_retriever.k = 3  # Top 3 keyword matches\n",
    "\n",
    "    return vector_retriever\n",
    "\n",
    "# 2Ô∏è‚É£ **Set Up RAG Chain**\n",
    "def setup_qa_chain(vector_retriever):\n",
    "    llm = ChatOllama(model=MODEL_NAME)  # Ollama for LLM\n",
    "    # **Hybrid Search Retriever (Merging Both)**\n",
    "    QUERY_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "            different versions of the given user question to retrieve relevant documents from\n",
    "            a vector database. By generating multiple perspectives on the user question, your\n",
    "            goal is to help the user overcome some of the limitations of the distance-based\n",
    "            similarity search. Provide these alternative questions separated by newlines.\n",
    "            Original question: {question}\"\"\",\n",
    "    )\n",
    "\n",
    "    retriever = MultiQueryRetriever.from_llm(\n",
    "        vector_retriever, llm, prompt=QUERY_PROMPT\n",
    "    )\n",
    "\n",
    "    logging.info(\"Retriever created.\")\n",
    "        # RAG prompt\n",
    "    template = \"\"\"Answer the question based ONLY on the following context:\n",
    "        {context}\n",
    "        Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    logging.info(\"Chain created successfully.\")\n",
    "\n",
    "    return chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Hybrid Search Chatbot (type 'exit' to quit)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/pull \"HTTP/1.1 200 OK\"\n",
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Vector database created.\n",
      "INFO:root:Retriever created.\n",
      "INFO:root:Chain created successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"ü§ñ Hybrid Search Chatbot (type 'exit' to quit)\")\n",
    "hybrid_retriever = create_hybrid_retriever()\n",
    "qa_chain = setup_qa_chain(hybrid_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. \"Which individual is considered the most skilled globally?\"', '2. \"Identify the top performer worldwide.\"', '3. \"Who stands out as the best globally?\"', '4. \"Find me the world\\'s leading figure.\"', '5. \"Which person reigns supreme internationally?\"']\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "query = \"Who best in the world\"\n",
    "result = qa_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Batman'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:langchain.retrievers.multi_query:Generated queries: ['Here are five different versions of the user question:', '1. **Clarified**: \"Who is considered the best among all individuals in the world?\"', '2. **Rephrased**: \"Who stands out as the top contender globally?\"', '3. **Alternative wording**: \"Who reigns supreme worldwide?\"', '4. **Narrowed down**: \"Who is generally regarded as the best individual on Earth?\"', '5. **Expanding context**: \"In various fields or aspects of life, who is widely acknowledged as the best in the world?\"']\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Batman\n",
      "\n",
      "Source Documents:\n",
      "1. question: Who best in the world?, answer: Batman is storangest can fight to everyone.\n",
      "2. question: What is the largest organ in the human body?, answer: The skin is the largest organ in the human body.\n",
      "3. question: How many continents are there on Earth?, answer: There are seven continents on Earth: Africa, Antarctica, Asia, Europe, North America, Oceania, and South America.\n"
     ]
    }
   ],
   "source": [
    "query = \"Who best in the world\"\n",
    "\n",
    "# Retrieve relevant documents\n",
    "retrieved_docs = hybrid_retriever.invoke(query)\n",
    "\n",
    "# Extract the text from the documents\n",
    "context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# Invoke the chain with the retrieved context\n",
    "result = qa_chain.invoke({\"context\": context, \"question\": query})\n",
    "\n",
    "# Print the answer and source documents\n",
    "print(\"Answer:\", result)\n",
    "print(\"\\nSource Documents:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"{i}. {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Batman'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: MultiQueryRetriever(retriever=VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x107e3c800>, search_kwargs={'k': 3}), llm_chain=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='You are an AI language model assistant. Your task is to generate five\\n            different versions of the given user question to retrieve relevant documents from\\n            a vector database. By generating multiple perspectives on the user question, your\\n            goal is to help the user overcome some of the limitations of the distance-based\\n            similarity search. Provide these alternative questions separated by newlines.\\n            Original question: {question}')\n",
       "           | ChatOllama(model='mistral-nemo')\n",
       "           | LineListOutputParser()),\n",
       "  question: RunnablePassthrough()\n",
       "}\n",
       "| ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based ONLY on the following context:\\n        {context}\\n        Question: {question}\\n    '), additional_kwargs={})])\n",
       "| ChatOllama(model='mistral-nemo')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=MODEL_NAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "a = llm.invoke(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
